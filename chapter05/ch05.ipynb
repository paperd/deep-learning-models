{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km-1djqqyZNN"
      },
      "source": [
        "# TPU Hardware Accelerator\n",
        "\n",
        "To vastly speed up processing, we can use the TPU available from the Google Colab cloud service.\n",
        "\n",
        "For a resource on TPUs in Colab, peruse:\n",
        "\n",
        "https://colab.research.google.com/notebooks/tpu.ipynb\n",
        "\n",
        "For a simple tutorial, peruse:\n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/classification_iris_data_with_keras.ipynb\n",
        "\n",
        "For a quick-start guide, peruse:\n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/tpu.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTJBEpakMn7Y"
      },
      "source": [
        "# Import **tensorflow** Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp4vJYyL0yIO"
      },
      "source": [
        "Import library and alias it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv9khXHkMowm"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLbKg64f6epN"
      },
      "source": [
        " # Enable TPU Runtime\n",
        " \n",
        "It’s very easy to enable the TPU in a Colab notebook:\n",
        "\n",
        "1.\tclick **Runtime** in the top left menu\n",
        "2.\tclick **Change runtime** type from the drop-down menu\n",
        "3.\tchoose **TPU** from the Hardware accelerator drop-down menu\n",
        "4.\tclick **SAVE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zw0agus5j9i"
      },
      "source": [
        "# TPU Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiFBZCD802sf"
      },
      "source": [
        "Set up TPU resolver and verify that it is running:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfCrv7JZ5UBZ"
      },
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjU574hu6r09"
      },
      "source": [
        "# Configure TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj5rpgi50-tk"
      },
      "source": [
        "Make devices on the cluster available to use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DymJTsYw5UdZ"
      },
      "source": [
        "tf.config.experimental_connect_to_cluster(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KIVw6yOMoQM"
      },
      "source": [
        "Initialize TPU devices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HD1YxbpMoZ6"
      },
      "source": [
        "tf.tpu.experimental.initialize_tpu_system(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLr05ZXSe2XW"
      },
      "source": [
        "# Create Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXKl0oB1CqN"
      },
      "source": [
        "Create a TPU strategy for this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i7x_C5se2ei"
      },
      "source": [
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVbs3K_u1OmX"
      },
      "source": [
        "Displayed is a list of available TPUs and other devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K1JODmF4Q9_"
      },
      "source": [
        "# Manual Device Placement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDRG6jYQ4UG_"
      },
      "source": [
        "After the TPU is initialized, we can use manual device placement to direct computation on a single TPU device:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOUszTr34RGf"
      },
      "source": [
        "a = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\n",
        "b = [[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]]\n",
        "with tf.device('/TPU:7'):\n",
        "  c = tf.matmul(a, b)\n",
        "I = [[1.0, 0.0], [0.0, 1.0]]\n",
        "with tf.device('/TPU:6'):\n",
        "  d = tf.matmul(c, I)   \n",
        "print('c device:', c.device)\n",
        "print(c)\n",
        "print('d device:', d.device)\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzvoKLn66d93"
      },
      "source": [
        "Multiply matrix a by matrix b and place the result in matrix c within the scope of TPU 7. Next, multiply matrix c by the Identity matrix and place the result in matrix d within the scope of TPU 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LItVHJJo5P6j"
      },
      "source": [
        "# Run a Computation in all TPU Cores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G7D03Zv49CC"
      },
      "source": [
        "To replicate a computation so it can run in all TPU cores, pass it to the *strategy.run* API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwcLIVFo49Ja"
      },
      "source": [
        "@tf.function\n",
        "def matmul_fn(x, y):\n",
        "  z = tf.matmul(x, y)\n",
        "  return z\n",
        "\n",
        "z = tpu_strategy.run(matmul_fn, args=(a, b))\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVUMUE785oF8"
      },
      "source": [
        "Create a function that multiplies two matrices. If eager behavior is enabled, make the function a `tf.function` or call `strategy.run` inside a `tf.function`. As of this writing, eager behavior is automatically enabled in Colab!\n",
        "\n",
        "Invoke the funciton with our `strategy.run` and all the cores obtain the same inputs (a, b) and do the matmul on each core independently. The outputs are the values from all the replicas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9joT3BgI64BW"
      },
      "source": [
        "# Digits Data Experiment\n",
        "\n",
        "The **digits** dataset is embedded in the **sklearn.datasets** package. It consists of 1,797 8x8 grayscale images. Each image is of a hand-written digit from 0-9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77UK-Pl_66l-"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWnKZaJ35O-U"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gGyiPsU69V_"
      },
      "source": [
        "Load digits data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VThNn1zZ2CcY"
      },
      "source": [
        "digits = load_digits()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_QGZnNx9KNS"
      },
      "source": [
        "## Get Keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSb1zaw91aHH"
      },
      "source": [
        "Get keys associated with the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVVsPZsS9KSP"
      },
      "source": [
        "digits.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDI_cUa79BUr"
      },
      "source": [
        "## Display an Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcdPowB81dkf"
      },
      "source": [
        "Display the first image from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lglv02qf871S"
      },
      "source": [
        "images = digits.images\n",
        "image = images[0]\n",
        "fig = plt.imshow(image, cmap='binary')\n",
        "fig = plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU0qVdk08bUK"
      },
      "source": [
        "## Create a Function to Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZV0W0w91jI4"
      },
      "source": [
        "The function splits the dataset into train and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u779BZvN3NlH"
      },
      "source": [
        "def load_data(digits, splits, random, scale):\n",
        "  X = digits.images\n",
        "  y = digits.target\n",
        "  x_train, x_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=splits, random_state=random)\n",
        "  x_train, x_test = x_train / scale, x_test / scale\n",
        "  return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nMcgsGN8o94"
      },
      "source": [
        "## Get Data into Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqIjIHDM1yhZ"
      },
      "source": [
        "Split into 2/3 train and 1/3 test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FCOdsptGlR9"
      },
      "source": [
        "splits, seed, scale = 0.33, 0, 255.0\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_data(\n",
        "    digits, splits, seed, scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c56w7ch9w80"
      },
      "source": [
        "Get target names and number of classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6fmi_c92gA9"
      },
      "source": [
        "target_names = digits.target_names\n",
        "num_classes = len(target_names)\n",
        "num_classes, target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZhbJS4W96Ez"
      },
      "source": [
        "## Prepare Data for TensorFlow Consumption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lOsNP_W2N19"
      },
      "source": [
        "Convert dataset to a TensorFlow consumable form:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mckjR86JG9hu"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZByH6OVf-D-T"
      },
      "source": [
        "Verify data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MxUcw3NJQET"
      },
      "source": [
        "for img, lbl in train_dataset.take(1):\n",
        "  print (img.shape, lbl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bas_QYd2-K6T"
      },
      "source": [
        "## Build the Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYQgL8RZ2W0c"
      },
      "source": [
        "Set batch and shuffle size. Build the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwRwj0E-HA6_"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "\n",
        "train_ds = train_dataset\\\n",
        " .shuffle(SHUFFLE_BUFFER_SIZE)\\\n",
        " .batch(BATCH_SIZE)\n",
        "test_ds = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg59Hq8w-QML"
      },
      "source": [
        "Get input shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uhJ_VynHJUv"
      },
      "source": [
        "for item in train_ds.take(1):\n",
        "  s = item[0].shape\n",
        "in_shape = s[1:]\n",
        "in_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx45beKV-Uhr"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stsQbXO7-ZFD"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkqpDU1bHY7Y"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ2TFCJ3-cBc"
      },
      "source": [
        "Create a function to build the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubEh1RrjF4Wt"
      },
      "source": [
        "def get_model():\n",
        "  return tf.keras.Sequential([\n",
        "    Flatten(input_shape=in_shape),\n",
        "    Dense(256, input_shape=in_shape, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-_Ce1wk-fPU"
      },
      "source": [
        "Create the model and compile within the TPU scope:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wFWlmOLF4Zi"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  model = get_model()\n",
        "  model.compile(\n",
        "      optimizer='adam',\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KhSIvXC7HQE"
      },
      "source": [
        "Creating the model in the TPUStrategy scope means we train the model on the TPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3fRlDgSaj1P"
      },
      "source": [
        "Inspect the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPt8NlwOaj8-"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfejAt4I-piU"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrNEzeH_2wXe"
      },
      "source": [
        "Train the model for sixty epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-WnOGM-FrAw"
      },
      "source": [
        "epochs = 60\n",
        "history = model.fit(train_ds, epochs=epochs,\n",
        "                    validation_data=(test_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRnzilXFCycp"
      },
      "source": [
        "# MNIST Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHiaOmtk25st"
      },
      "source": [
        "Load MNIST train and test sets as TFDS objects:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpN20BIeW58c"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "train, info = tfds.load(name='mnist', split='train',\n",
        "                        as_supervised=True, try_gcs=True,\n",
        "                        with_info=True, shuffle_files=True)\n",
        "test = tfds.load(name='mnist', split='test',\n",
        "                        as_supervised=True, try_gcs=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDF6R7ckYcPa"
      },
      "source": [
        "## Get Metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CxIl9Uq3ct4"
      },
      "source": [
        "Display information about the dataset with the **info** object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u0Y0CVwYcTs"
      },
      "source": [
        "info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT1E37A-YXQC"
      },
      "source": [
        "## Get Number of Images and Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBRlBlf_3ofP"
      },
      "source": [
        "Number of images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7Ie0oYDYPxq"
      },
      "source": [
        "num_train_img = info.splits['train'].num_examples\n",
        "num_test_img = info.splits['test'].num_examples\n",
        "print ('train images:', num_train_img)\n",
        "print ('test images:', num_test_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaOEUEbcd5hH"
      },
      "source": [
        "Number of classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m1lKJ43d5on"
      },
      "source": [
        "mnist_classes = info.features['label'].num_classes\n",
        "mnist_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRd0qZWUYmbz"
      },
      "source": [
        "## Display Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLO1N5K63unP"
      },
      "source": [
        "Display some examples with the **show_examples** function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RX5AO4rYP2S"
      },
      "source": [
        "fig = tfds.show_examples(train, info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNI97rrGXrAQ"
      },
      "source": [
        "## Create Scaling Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMLZGoXP30zQ"
      },
      "source": [
        "Create a function that scales TFDS data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhjbVxF0XrEi"
      },
      "source": [
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32) / 255.0\n",
        "  return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM8jZk0SW5_-"
      },
      "source": [
        "## Build the Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9fIlGxdW6Fl"
      },
      "source": [
        "BATCH_SIZE = 200\n",
        "SHUFFLE_SIZE = 10000\n",
        "\n",
        "train_dataset = train.map(scale)\\\n",
        "  .shuffle(SHUFFLE_SIZE).repeat()\\\n",
        "  .batch(BATCH_SIZE).prefetch(1)\n",
        "test_dataset = test.map(scale)\\\n",
        "  .batch(BATCH_SIZE).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN7is6EZSd2V"
      },
      "source": [
        "Only shuffle and **repeat** the training dataset. The advantage to an infinite dataset for training is to avoid the potential last partial batch in each epoch, so users don't need to think about scaling the gradients based on the actual batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgv3FttwW6KX"
      },
      "source": [
        "## Create Step Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytcdphQSskyp"
      },
      "source": [
        "When repeating data, include the steps per epoch and validation steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q_cgcKAW6PO"
      },
      "source": [
        "steps_per_epoch = num_train_img // BATCH_SIZE\n",
        "validation_steps = num_test_img // BATCH_SIZE\n",
        "steps_per_epoch, validation_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdZtVtgyX3Qv"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P18bO0SsclHW"
      },
      "source": [
        "Get input shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dji-hKQkclMc"
      },
      "source": [
        "for item in train_dataset.take(1):\n",
        "  s = item[0].shape\n",
        "mnist_shape = s[1:]\n",
        "mnist_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXT60UYHaIFW"
      },
      "source": [
        "Create function that builds the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuyzMVh5aRC3"
      },
      "source": [
        "def create_model():\n",
        "  return Sequential([\n",
        "    Flatten(input_shape=mnist_shape),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(mnist_classes, activation='softmax')\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv6qN6ijX3Zn"
      },
      "source": [
        "Clear previous models and generate seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tTt2j1yX3d7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHuY2A98X-WA"
      },
      "source": [
        "## Train within TPU Scope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omKcxT6XOVl6"
      },
      "source": [
        "Import library for the loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Hc96hlOXDQ"
      },
      "source": [
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVc3hRMUOYf4"
      },
      "source": [
        "Within TPU strategy scope, create model and compile:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcpZGPiKOYmg"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  model = create_model()\n",
        "  model.compile(\n",
        "      optimizer='adam',\n",
        "      steps_per_execution = 50,\n",
        "      loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnu6KZk11YXR"
      },
      "source": [
        "Creating the model in the TPUStrategy scope means we train the model on the TPU.\n",
        "Experiment with `steps_per_execution`. Anything between 2 and `steps_per_epoch` could help here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crGFUyTqOjQw"
      },
      "source": [
        "Train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPG8o6LuX-Zy"
      },
      "source": [
        "history = model.fit(\n",
        "    train_dataset, epochs=5,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=test_dataset,\n",
        "    validation_steps=validation_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXa6E0lBjlyI"
      },
      "source": [
        "# Fashion-MNIST Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgLk1PBT8vcP"
      },
      "source": [
        "**Fashion-MNIST** is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. The dataset is intended to serve as a direct drop-in replacement of the original MNIST dataset for benchmarking machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Llh-1KO9g86"
      },
      "source": [
        "Load Fashion-MNIST as a tf.keras dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX6qGzLrGceS"
      },
      "source": [
        "fashion_train, fashion_test = tf.keras.datasets\\\n",
        "  .fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sk-RRbQK5L8"
      },
      "source": [
        "Create image and label sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ep5N31dHDgU"
      },
      "source": [
        "train_img, train_lbl = fashion_train\n",
        "test_img, test_lbl = fashion_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlEo6RUVAuai"
      },
      "source": [
        "Add color (grayscale) dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6FUll_iAugn"
      },
      "source": [
        "fashion_train_img = np.expand_dims(train_img, -1)\n",
        "fashion_test_img = np.expand_dims(test_img, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQYHpiyhKtne"
      },
      "source": [
        "Inspect image tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9m_rGG6HpCR"
      },
      "source": [
        "fashion_train_img.shape, fashion_test_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CyPm4O_Kwxd"
      },
      "source": [
        "Create a function to convert tensors to float:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkSgct0QI-fD"
      },
      "source": [
        "def float_it(x):\n",
        "  return x.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfs41SxfI-ux"
      },
      "source": [
        "fash_train_img, fash_train_lbl = float_it(\n",
        "    fashion_train_img), float_it(train_lbl)\n",
        "fash_test_img, fash_test_lbl = float_it(\n",
        "    fashion_test_img), float_it(test_lbl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnAUJv48D_4E"
      },
      "source": [
        "## Train with TPU Scope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "425btgXeM1fr"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhwmkeuuD_-e"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization,\\\n",
        "  Conv2D, MaxPooling2D, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofwcIfU9D5qY"
      },
      "source": [
        "Create a function to build the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCfEQY6Y884n"
      },
      "source": [
        "def create_model():\n",
        "  return Sequential([\n",
        "    BatchNormalization(input_shape=(28,28,1)),\n",
        "    Conv2D(64, (5, 5), padding='same', activation='elu'),\n",
        "    MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(128, (5, 5), padding='same', activation='elu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(256, (5, 5), padding='same', activation='elu'),\n",
        "    MaxPooling2D(pool_size=(2, 2), strides=(2,2)),    \n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='elu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1ff96L5M6Ix"
      },
      "source": [
        "Clear and seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUz-76k0GjdN"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cv8ZvQQM8nc"
      },
      "source": [
        "Create and compile the model within TPU scope:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTh-nyT0887O"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  model = create_model()\n",
        "  model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD9nZZj3NC1r"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th6cxrK8NEV4"
      },
      "source": [
        "Train the model based on Fashion-MNIST data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdsAvCnTImng"
      },
      "source": [
        "history = model.fit(fash_train_img, fash_train_lbl,\n",
        "    epochs=17, steps_per_epoch=60,\n",
        "    validation_data=(fash_test_img, fash_test_lbl),\n",
        "    validation_freq=17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjOmLiLePKT3"
      },
      "source": [
        "Evaluate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPHIE3_C9AWG"
      },
      "source": [
        "loss, acc = model.evaluate(fash_test_img, fash_test_lbl)\n",
        "print ('loss:', loss)\n",
        "print ('accuracy:', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHp_x8BZ_esm"
      },
      "source": [
        "## Save the Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2amY3KT_i71"
      },
      "source": [
        "We can preserve the weights from the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5DoadWE_ezf"
      },
      "source": [
        "model.save_weights('./fashion_mnist.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhyZCZ2NQ8Hq"
      },
      "source": [
        "## Make Inferences\n",
        "\n",
        "Let’s see how well the model can predict fashion categories!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MfX_9RzRY-2"
      },
      "source": [
        "Get label names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zBuH-VGRZD8"
      },
      "source": [
        "class_labels = ['t_shirt', 'trouser', 'pullover', 'dress',\n",
        "                'coat', 'sandal', 'shirt', 'sneaker',\n",
        "                'bag', 'ankle_boots']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uymJEOh4RlCr"
      },
      "source": [
        "Create a new model from the saved weights of the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZB8S3cdRlI3"
      },
      "source": [
        "new_model = create_model()\n",
        "new_model.load_weights('./fashion_mnist.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bYSM6Hy0AlE"
      },
      "source": [
        "Get 40 prediction arrays from the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1N0Pb6w0Foo"
      },
      "source": [
        "preds = new_model.predict(fash_test_img)[:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc63fqL39jsd"
      },
      "source": [
        "Transform prediction arrays to predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKUaSfUn8HHo"
      },
      "source": [
        "pred_40 = [tf.argmax(i).numpy() for i in preds]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUQyBRH632BR"
      },
      "source": [
        "Get images and labels for display:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtGs7P2-32Gb"
      },
      "source": [
        "images, labels = [], []\n",
        "for i in range(40):\n",
        "  img = tf.squeeze(fash_test_img[i])\n",
        "  images.append(img)\n",
        "  labels.append(int(fash_test_lbl[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olTI1vpA93tn"
      },
      "source": [
        "Create a function to display predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Li6bK5p2-xY"
      },
      "source": [
        "def display_test(feature, target, num_images,\n",
        "                 n_rows, n_cols, cl, p):\n",
        "  for i in range(num_images):\n",
        "    plt.subplot(n_rows, 2*n_cols, 2*i+1)\n",
        "    plt.imshow(feature[i], cmap='nipy_spectral')\n",
        "    pred = cl[p[i]]\n",
        "    actual = cl[int(target[i])]\n",
        "    title_obj = plt.title(actual + ' (' +\\\n",
        "                          pred + ') ')\n",
        "    if pred == actual:\n",
        "      title_obj\n",
        "    else:\n",
        "      plt.getp(title_obj, 'text')\n",
        "      plt.setp(title_obj, color='r')\n",
        "    plt.tight_layout()\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgMIUXhf94MG"
      },
      "source": [
        "Invoke the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwkrOq0Y3hUJ"
      },
      "source": [
        "num_rows, num_cols = 10, 4\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(20, 20))\n",
        "display_test(images, labels, num_images, num_rows,\n",
        "             num_cols, class_labels, pred_40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwyE8V2yEJ-5"
      },
      "source": [
        "# Flowers Experiment\n",
        "\n",
        "TPUs are very fast. So the training data streaming into the model must keep pace with the training speed of the model to fully leverage the power of TPUs. The preferred method for TPU usage is to store data into the protobuf-based TFRecord format.\n",
        "\n",
        "The flowers dataset is stored on Google Cloud Storage (GCS). To fully apply the parallelism TPUs offer and to avoid bottlenecking on data transfer, we read data as TFRecord files with approximately 230 images per file. We use tf.data.experimental.AUTOTUNE to optimize different parts of input loading.\n",
        "\n",
        "A nice site on TPUs in Colab:\n",
        "\n",
        "https://colab.research.google.com/notebooks/tpu.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipe4ha3CLVkN"
      },
      "source": [
        "## Read Flowers Data as TFRecord Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq-nTdr8KJMK"
      },
      "source": [
        "Read TFRecord files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE60b13BK3oN"
      },
      "source": [
        "piece1 = 'gs://flowers-public/'\n",
        "piece2 = 'tfrecords-jpeg-192x192-2/*.tfrec'\n",
        "TFR_GCS_PATTERN = piece1 + piece2\n",
        "tfr_filenames = tf.io.gfile.glob(TFR_GCS_PATTERN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqDSfmsrLDD1"
      },
      "source": [
        "Get the number of buckets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxbYQjknLDKn"
      },
      "source": [
        "num_images = len(tfr_filenames)\n",
        "print ('Pattern matches {} image buckets.'.format(num_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26SbrmZBLJ1M"
      },
      "source": [
        "Display all 16 buckets (TFRecord files):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIoN1511KJSI"
      },
      "source": [
        "filenames_tfrds = tf.data.Dataset.list_files(TFR_GCS_PATTERN)\n",
        "for filename in filenames_tfrds.take(16):\n",
        "  print (filename.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDRjEijXLR6W"
      },
      "source": [
        "## Set Parameters for Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yjl2xWaLjd3"
      },
      "source": [
        "Set parameters for image resizing, pipelining, and number of epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpyOKjqNLSAj"
      },
      "source": [
        "IMAGE_SIZE = [192, 192]\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_SIZE = 100\n",
        "EPOCHS = 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WcTgyxQKm9T"
      },
      "source": [
        "Set parameters for data splits and labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_t89SgoKjYY"
      },
      "source": [
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rinppV2iLwE9"
      },
      "source": [
        "Create data splits, validation steps, and steps per epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho4Xtv-xKWpL"
      },
      "source": [
        "split = int(len(tfr_filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = tfr_filenames[split:]\n",
        "validation_filenames = tfr_filenames[:split]\n",
        "print ('Splitting dataset into {} training files and {} '\\\n",
        "        'validation files'\\\n",
        "       .format(len(tfr_filenames), len(training_filenames),\n",
        "               len(validation_filenames)), end = ' ')\n",
        "print ('with a batch size of {}.'.format(BATCH_SIZE))\n",
        "\n",
        "validation_steps = int(3670 // len(tfr_filenames) *\\\n",
        "                       len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(tfr_filenames) *\\\n",
        "                      len(training_filenames)) // BATCH_SIZE\n",
        "print ('There are {} batches per training epoch and {} '\\\n",
        "       'batches per validation run.'\\\n",
        "       .format(BATCH_SIZE, steps_per_epoch, validation_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLsA8YopKfkR"
      },
      "source": [
        "## Create Functions to Load and Process TFRecord Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0IXWgE_L6rX"
      },
      "source": [
        "Create a function to parse a TFRecord file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqXzGINLKfpc"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "  features = {\n",
        "      'image': tf.io.FixedLenFeature([], tf.string),\n",
        "      'class': tf.io.FixedLenFeature([], tf.int64)\n",
        "  }\n",
        "  example = tf.io.parse_single_example(example, features)\n",
        "  image = tf.image.decode_jpeg(example['image'], channels=3)\n",
        "  image = tf.cast(image, tf.float32) / 255.0 \n",
        "  image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "  class_label = example['class']\n",
        "  return image, class_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svpfsjh9MA2f"
      },
      "source": [
        "The function accepts an example from a TFRecord file. A dictionary holds datatypes common to TFRecords. The tf.string parameter converts the image to byte strings (list of bytes). The tf.int64 parameter converts the class label to a 64-bit integer scalar value. The example is parsed into (image, label) tuples. The image element, a JPEG-encoded image, is decoded into a uint8 image tensor. The image tensor is scaled to \\[0, 1\\] range for faster training. It is then reshaped to a standard size for model consumption. The class label element is cast to a scalar. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YzWm22zMaJg"
      },
      "source": [
        "Create a function to load TFRecord files as tf.data.Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZRhzJ2mMaRH"
      },
      "source": [
        "def load_dataset(filenames):\n",
        "  option_no_order = tf.data.Options()\n",
        "  option_no_order.experimental_deterministic = False\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "      filenames, num_parallel_reads=AUTO)\n",
        "  dataset = dataset.with_options(option_no_order)\n",
        "  dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebei-3vsMfmX"
      },
      "source": [
        "The function accepts TFRecord files. For optimal performance, code is included to read from multiple TFRecord files at once. The options setting allows order-altering optimizations. As such, **n** files are read in parallel and data order is disregarded in favor of reading speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6v7qTNjR2so"
      },
      "source": [
        "Create a function to augment training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GnwBxGBQJ2u"
      },
      "source": [
        "def data_augment(image, label):\n",
        "  modified = tf.image.random_flip_left_right(image)\n",
        "  modified = tf.image.random_saturation(modified, 0, 2)\n",
        "  return modified, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EHqqcTcK2yU"
      },
      "source": [
        "Create a function to build an input pipeline from TFRecord files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDIeyYaGK25j"
      },
      "source": [
        "def get_batched_dataset(filenames, train=False):\n",
        "  dataset = load_dataset(filenames)\n",
        "  dataset = dataset.cache()\n",
        "  if train:\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(SHUFFLE_SIZE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(AUTO)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4KIfBwBLNZ3"
      },
      "source": [
        "The function accepts TFRecord files and calls the `load_dataset` function. The function continues by building an input pipeline by caching, repeating, shuffling, batching and prefetching the dataset. Repeating and shuffling are only mapped to training data. We follow best practices by repeating and shuffling only the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0cMjEAgMKUw"
      },
      "source": [
        "## Create Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fXR3mG-MNCu"
      },
      "source": [
        "Instantiate the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84qIjfibLNf-"
      },
      "source": [
        "training_dataset = get_batched_dataset(\n",
        "    training_filenames, train=True)\n",
        "validation_dataset = get_batched_dataset(\n",
        "    validation_filenames, train=False)\n",
        "training_dataset, validation_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndBDGgl4MtvB"
      },
      "source": [
        "Display an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJf3YlUnMt4Q"
      },
      "source": [
        "for img, lbl in training_dataset.take(1):\n",
        "  plt.axis('off')\n",
        "  plt.title(CLASSES[lbl[0].numpy()])\n",
        "  fig = plt.imshow(img[0])\n",
        "  tfr_flower_shape = img.shape[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KixeTON5My12"
      },
      "source": [
        "## Model Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5IrMcnCNMOQ"
      },
      "source": [
        "Clear and seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1f05BKfMy9w"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6kTIPVxLWwN"
      },
      "source": [
        "Create model function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOo5S3T1LW2N"
      },
      "source": [
        "def create_model():\n",
        "  return Sequential([\n",
        "    Conv2D(32, (3, 3), activation = 'relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(num_classes, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKA4r82dLy9f"
      },
      "source": [
        "Create and compile model within TPU scope:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elHwCq5nLzDf"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  flower_model = create_model()\n",
        "  flower_model.compile(\n",
        "      optimizer='adam',\n",
        "      loss=tf.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9YN7ualFUNy"
      },
      "source": [
        "Train model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW1TA2q_EKEo"
      },
      "source": [
        "history = flower_model.fit(training_dataset, epochs=EPOCHS,\n",
        "                    verbose=1, steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=validation_steps, \n",
        "                    validation_data=validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq22XZkUOrdP"
      },
      "source": [
        "## Make Inferences\n",
        "\n",
        "Let’s see how well the model can predict fashion categories!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zh8Biu_OvtQ"
      },
      "source": [
        "Grab 40 predictions from the validation (test) set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0qNnOQKOrkJ"
      },
      "source": [
        "preds = flower_model.predict(validation_dataset)[:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es_DCVdnOrrR"
      },
      "source": [
        "Convert prediction arrays to scalar prediction values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe_u2OxXOrw2"
      },
      "source": [
        "pred_40 = [tf.argmax(i).numpy() for i in preds]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fib7EVvQOr-j"
      },
      "source": [
        "Create images and labels for display:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqlBikcSOsDx"
      },
      "source": [
        "images, labels = [], []\n",
        "for img, lbl in validation_dataset.take(1):\n",
        "  for i in range(40):\n",
        "    actual_img = tf.squeeze(img[i])\n",
        "    images.append(actual_img)\n",
        "    labels.append(lbl[i].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI1dsxo6PjKN"
      },
      "source": [
        "Create a function to display predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfGy8_OpPjRQ"
      },
      "source": [
        "def display_test(feature, target, num_images,\n",
        "                 n_rows, n_cols, cl, p):\n",
        "  for i in range(num_images):\n",
        "    plt.subplot(n_rows, 2*n_cols, 2*i+1)\n",
        "    plt.imshow(feature[i])\n",
        "    pred = cl[p[i]]\n",
        "    actual = cl[int(target[i])]\n",
        "    title_obj = plt.title(actual + ' (' +\\\n",
        "                          pred + ') ')\n",
        "    if pred == actual:\n",
        "      title_obj\n",
        "    else:\n",
        "      plt.getp(title_obj, 'text')\n",
        "      plt.setp(title_obj, color='r')\n",
        "    plt.tight_layout()\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPmPEM4iRQD6"
      },
      "source": [
        "Invoke function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozni5QTCRQJ9"
      },
      "source": [
        "num_rows, num_cols = 10, 4\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(20, 20))\n",
        "display_test(images, labels, num_images, num_rows,\n",
        "             num_cols, CLASSES, pred_40)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}